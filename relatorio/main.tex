\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage{nameref}

\usepackage[english]{babel}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Deep Learning for Skin Lesion Segmentation and Classification: ISIC Challenge 2018}

\author{\IEEEauthorblockN{Fred Guth}}

\maketitle

\begin{abstract}
This report describes our approach for the ISIC Challenge 2018 - Skin Lesion Analysis Towards Melanoma Detection. For Part 1 - Lesion Segmentation, we developed a U-net based convolutional neural network pretrained with the ImageNet dataset\cite{imagenet} and applied several data augmentation and hyperparameters optimization strategies, obtaining threshold(0.65) jaccard of 0.775. For Part 3 - Lesion Classification, we developed an ensemble strategy that leverages pretrained convolutional networks with our results from Part 1, obtaining an online score of xxxx. 
\end{abstract}

\section{Introduction}

According to the World Health Organization, between 2 and 3 million non-melanoma skin cancers and 132,000 melanoma skin cancers occur globally each year\cite{who}. Despite representing less than 6.5\% of all skin cancers, melanomas are the most dangerous type, accounting for aproximately 75\% of all skin cancer related deaths\cite{who, nature}.  Visual inspection still is the most common diagnostic technique and early detection is critical to increase survival expectancy.

Deep convolutional neural networks (CNNs) already exceed human performance in visual classification\cite{fei}.  In an attempt to improve the scalability of diagnostic expertise, CNNs have been developed to locate and classify skin cancers in images with dermatologist-level accuracy\cite{nature}.

Dermoscopy is a technique for examination of skin lesions that, with proper training, increase dianostic accuracy from 60\% (unaided expert visual inspection) to 75\%-84\%\cite{isic}. The International Skin Imaging Collaboration (ISIC) has a large-scale publicly acessible dataset of more than 20,000 dermoscopy images and host an annual benchmark challenge on dermoscopic image analysis since 2016.  The challenge comprise 3 tasks of lesion analysis: Part 1 - Segmentation, Part 2 - Dermoscopic feature extraction, Part 3 - Classification. 

In this paper, we describe our approach for the ISIC Challenge 2018:
\begin{itemize}
\item \textbf{Section \ref{segmentation}} describes our methodology for \textbf{lesion segmentation};
\item \textbf{Section \ref{classification}} describes our methodology for \textbf{lesion classification};
\end{itemize}


\section{Lesion Segmentation}
\label{segmentation}
\subsection{Computational Resources and Development Framework\label{resources}}
We used a Paperspace GPU Cloud service for running all our experiments. The instance server used had: 8 cores CPU with 30GB of RAM, Quadro P4000 8 core GPU with 8GB of RAM. 
The code was developed in PyTorch and Fast.ai\cite{fastai} on Jupyter Notebooks.
\subsection{Data and Augmentation}
We used “ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection” grand challenge datasets \cite{codella, ham} and no aditional external data. All images were first resized to 128x128 pixels, 256x256 and 512x512; and preprocessed to adjust color balance. Random transformations on input images to agument the dataset were made: dihedral transformation, rotation (up to 44 degrees), zooming (up to 1.05), fliping and random lightining changes. The official training dataset was then splited in 3-folds of training and validation datasets.  

\subsection{Model: Unet34}
Introduced in 2015, U-net is an encoder-decoder architecture designed for biomedical image segmentation\cite{olaf}. In an U-net the output is an image with the same dimension of the input, but with one channel.  The encoder path is a typical CNN, where each downsampling step doubles the number of feature channels. But what makes this architecture unique is the decoder path, where each upsampling step input is a concatenation of the output of the previous step with the output of the corresponding (same height) downsampling step. This strategy enables precise localization with a very simple network. 

Resnet is a very successful architecture in several visual recognizing tasks\cite{he}. It mitigates the degradation problem that happens when very deep networks starts converging. Instead of learning a direct mapping $H(x) = y$, it learns the residual function  $F(x) = H(x) — x$, which can be reframed into $H(x) = F(x)+x = y$, where $F(x)$ is a stack of non-linear layers and $x$ is the identity function(input=output). The formulation of $F(x)+x$ can be implemented by feedforward neural networks with “shortcut connections” (see Figure \ref{shortcut}).

Unet34 is the idea of using a pretrained Resnet34 model as an Unet encoder path\cite{fastai}. First, everything from the adaptive pooling onwards is removed, keeping only Resnet backbone. Then we save the output of results of 2nd, 4th, 5th, and 6th blocks. During the upsampling we concat the output of those with the ouputs of upsampling steps. We used Adam optimizer and Binary Cross Entropy with Logits as the loss function.

\subsection{Experiments and Training}
Our training strategy was to first train the model with 128x128 images and transfer this learning to train the same model with images with 256 x 256 images. Initially we thought of using the same strategy to go from 256x256 to 512 x 512 images, but due to GPU memory constraints, we didn't manage to acccomplish this last step.

\begin{figure}
\centering
\subfloat[Learning Rate Optimization]{\includegraphics[width=.5\columnwidth]{LR_find.png}}
\subfloat[cyclical learning rate policy]{\includegraphics[width=.5\columnwidth]{lr_cycle.png}}\hfil
\subfloat[superconvergence]{\includegraphics[width=\columnwidth]{accuracy_epoch.png}}
\caption{colocar aqui}\label{lr_find_chart}
\end{figure}

The training procedure was the same on 128x128 and 256x256:
\begin{enumerate}
  \item Freeze the first layer group.
  \item \label{lr_find}Define the optimal learning rate with the method proposed by \cite{leslie} and implemented by \cite{fastai}, where one batch is trained with different learning rates, starting at very low and linearly increasing it at every iteration and ploting a chart of the learning rate versus loss (see figure \ref{lr_find_chart}).
  \item \label{superconvergence}We use the cyclical learning rate method, also proposed by \cite{leslie}, to obtain training convergence in only 30 epochs (what is called superconvergence).
  \item Unfreeze the model, keeping only the batch normalization layers frozen, and repeat steps \ref{lr_find} and \ref{superconvergence}.
\end{enumerate}

(explain 3-fold pretrained with best 256x256)

We have tried to change the loss function to make it more similar to the avaliation criteria. As jaccard is not differentiable, we used the differentiable soft jaccard variation. Despite of that, we couldn't make it work better than the Binary Cross Entropy with Logits loss function. 

\subsection{Segmentation Results}

\begin{figure}
\centering
\subfloat[Good Result Sample\label{good}]{\includegraphics[width=\columnwidth]{good_result.png}}\hfil
\subfloat[Bad Result Samples]{\includegraphics[width=\columnwidth]{bad_results.png}}
\caption{??.}\label{result_samples}
\end{figure}

The best result within our validation set was obtained using the 3-fold ensemble.  It scores 85.39\% Jaccard index and 78.43\% Threshold Jaccard index (with cut at 65\%). Surprisingly, it did not score so well with the online score and given official validation set, scoring 71.4\%.  The small size of the official validation set and the threshold at 65\% may be the reason for that (one change in prediction from 64\% or 66\% Jaccard makes a huge difference in the average threshold jaccard index). Our best online score with the official validation set was 75.5\%. 

Visually the best segmentations are almost indentical to the ground truth (see \ref{good}). But we can learn even more from our mistakes. Analysing the worse segmentations there are cases where as non specialists is hard to say if the algorithm was wrong or the ground truth was; there are cases where our algorithm got confused by the pen marker or the glass used by the doctor; and it is clear that in general it doesn't do a good job when the lesion is small relative to the overall image. 

\section{Lesion Classification}
\label{classification}
\subsection{Computational Resources and Development Framework}
See \ref{resources}.
\subsection{Data and Augmentation}
We used the ISIC 2018 Challenge official dataset with no aditional external data. All images were first resized to 224x224 and preprocessed to adjust color balance. Random transformations on input images to agument the dataset: dihedral transformation, rotate (up to 35 degrees), zoom (up to 1.05x), flip and random lighting changes. The training dataset was then splited in 3-folds of training and validation datasets.  

\subsection{Model}
We used two similar architetures: Resnet50 and Resnext101. Both architectures habe been very succesful in image classification problems (cite?). 

\subsection{Training}


\begin{figure}
\centering
\subfloat[Learning Rate Optimization]{\includegraphics[width=.5\columnwidth]{LR_find.png}}
\subfloat[cyclical learning rate policy]{\includegraphics[width=.5\columnwidth]{lr_cycle.png}}\hfil
\subfloat[superconvergence]{\includegraphics[width=\columnwidth]{accuracy_epoch.png}}
\caption{colocar aqui}\label{lr_find_chart}
\end{figure}

The training procedure was the same on 128x128 and 256x256:
\begin{enumerate}
  \item Freeze the first layer group.
  \item \label{lr_find}Define the optimal learning rate with the method proposed by \cite{leslie} and implemented by \cite{fastai}, where one batch is trained with different learning rates, starting at very low and linearly increasing it at every iteration and ploting a chart of the learning rate versus loss (see figure \ref{lr_find_chart}).
  \item \label{superconvergence}We use the cyclical learning rate method, also proposed by \cite{leslie}, to obtain training convergence in only 30 epochs (what is called superconvergence).
  \item Unfreeze the model, keeping only the batch normalization layers frozen, and repeat steps \ref{lr_find} and \ref{superconvergence}.
\end{enumerate}

(explain 3-fold pretrained with best 256x256)

\subsection{Segmentation Results}

\begin{figure}
\centering
\subfloat[Good Result Sample\label{good}]{\includegraphics[width=\columnwidth]{good_result.png}}\hfil
\subfloat[Bad Result Samples]{\includegraphics[width=\columnwidth]{bad_results.png}}
\caption{??.}\label{result_samples}
\end{figure}

The best result within our validation set was obtained using the 3-fold ensemble.  It scores 85.39\% Jaccard index and 78.43\% Threshold Jaccard index (with cut at 65\%). Surprisingly, it did not score so well with the online score and given official validation set, scoring 71.4\%.  The small size of the official validation set and the threshold at 65\% may be the reason for that (one change in prediction from 64\% or 66\% Jaccard makes a huge difference in the average threshold jaccard index). Our best online score with the official validation set was 75.5\%. 

Visually the best segmentations are almost indentical to the ground truth (see \ref{good}). But we can learn even more from our mistakes. Analysing the worse segmentations there are cases where as non specialists is hard to say if the algorithm was wrong or the ground truth was; there are cases where our algorithm got confused by the pen marker or the glass used by the doctor; and it is clear that in general it doesn't do a good job when the lesion is small relative to the overall image. 




\section{Discussão e Conclusões}
Neste trabalho, implementamos dois algoritmos para rastreamento visual de objetos em vídeo. O algoritmo KCF apresenta bons resultados de acurácia e robustez.  Entretanto, mostramos que seu modelo pode melhorar se incorporar incerteza, o que pode ser feito com um filtro de Kalman. Ao \textit{amortecer}  o resultado do KCF com um filtro de Kalman, tivemos resultados de robustez entre 60 e 90\% melhores, com perdas de acurácia menores que 12\%, esse resultado serve de inspiração para melhorias na implementação do rastreador KCF da OpenCV. 

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig.jpg}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

\selectlanguage{english}
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
